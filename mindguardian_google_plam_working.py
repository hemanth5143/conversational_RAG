# -*- coding: utf-8 -*-
"""mindguardian_google_plam_working.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rMkkfK1Fj6oCdUwVlVmlSLTs_iPnzcXl
"""

pip install sentence-transformers gradio chromadb langchain langchain_core langchain_community

import json
from sentence_transformers import SentenceTransformer
import gradio as gr
import os
import chromadb
from chromadb.utils import embedding_functions
from langchain.chains import LLMChain
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_core.messages import SystemMessage
from langchain.chains.conversation.memory import ConversationBufferWindowMemory

with open('/content/Alexander_Street_shareGPT_2.0.json', 'r') as file:
    dataset = json.load(file)
print("Dataset loaded successfully.")

chroma_client = chromadb.Client()
print("ChromaDB client initialized.")

# Define the embedding function
embedding_model = SentenceTransformer("nomic-ai/nomic-embed-text-v1.5", trust_remote_code=True)

# Define a custom embedding function
class CustomEmbeddingFunction:
    def __call__(self, input: str) -> list:
        # Use the embedding model to encode the input text
        embedding = embedding_model.encode(input)
        return embedding.tolist()

# Create an instance of the custom embedding function
embedding_func = CustomEmbeddingFunction()
print("Embedding function defined.")

# Create a collection in ChromaDB
collection = chroma_client.create_collection(
    name="mindguardian_collection",
    embedding_function=embedding_func,
    metadata={"hnsw:space": "cosine"}
)
print("ChromaDB collection created.")

from more_itertools import chunked

import uuid

def store_embedded_data_in_chromadb(dataset, batch_size=10):
    global_id = 0  # Start a global counter
    for batch in chunked(dataset, batch_size):
        ids = []
        documents = []
        metadatas = []

        for data in batch:
            merged_text = data['input'] + " " + data['output']
            ids.append(str(global_id))  # Use the global counter for unique IDs
            documents.append(merged_text)
            metadatas.append(data)  # Store original data as metadata
            global_id += 1  # Increment the global counter

        # Add data to the ChromaDB collection
        collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )
    print("Data stored in ChromaDB.")

store_embedded_data_in_chromadb(dataset)

from langchain.llms import GooglePalm

token = "AIzaSyCmToFLGJ91GWKTo2tkLvJLOSWdp0vkuYI"
llm_google_palm = GooglePalm(google_api_key=token, temperature=0.1, max_tokens= 100)

system_prompt = "<s>[INST] You are an expert mental health counseling chatbot named Mindguardian. You provide professional mental health counseling to users. [/INST]"
conversational_memory_length = 10
memory = ConversationBufferWindowMemory(k=conversational_memory_length, memory_key="chat_history", return_messages=True)
print("System prompt and memory initialized.")

def query_chromdb(user_query):
    # Generate the query vector from the user's input
    query_vector = embedding_model.encode(user_query).tolist()
    response = collection.query(
        query_embeddings=[query_vector],
        n_results=1,
        include=["metadatas"]  # Include embeddings in the response
    )
    return response

# Example usage
response = query_chromdb("I've been feeling a bit off. I sometimes find it hard to focus and concentrate on tasks, but it doesn't happen too often. I have been feeling a bit disconnected from my friends, but we still hang out and talk normally. My sleeping habits haven't changed much")
print(response['metadatas'])

def query_llm(user_question, _):
    try:
        print(f"Received user question: {user_question}")
        context = query_chromdb(user_question)

        # Construct a chat prompt template using various components
        prompt = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=system_prompt
                ),
                MessagesPlaceholder(
                    variable_name="chat_history"
                ),
                SystemMessage(
                    content=f"<s>[INST] Use this context only if relevant to user query: {context} [/INST]"
                ),
                HumanMessagePromptTemplate.from_template(
                    "<s>[INST] User query: {human_input} [/INST]"
                ),
            ]
        )
        print("Prompt constructed.")

        # Create a conversation chain using the LangChain LLM
        conversation = LLMChain(
            llm=llm_google_palm,  # Use Mistral 7B model
            prompt=prompt,
            verbose=False,
            memory=memory,
        )
        print("LLMChain initialized.")
        response = conversation.predict(human_input=user_question)
        print(f"LLM response: {response}")

        return response
    except Exception as e:
        print(f"Error in query_llm: {e}")
        return "Sorry, something went wrong. Please try again."

# Default message for the chatbot
default_message = """I'm MindGuardian, a mental health counseling chatbot. How can I help you?"""

gradio_interface = gr.ChatInterface(
    query_llm,
    chatbot=gr.Chatbot(value=[[None, default_message]]),
    textbox=gr.Textbox(placeholder="Type your query", container=False, scale=7),
    title="Mindguardian, a mental health counseling chatbot",
    theme='gradio/base',
    retry_btn=None,
    undo_btn="Delete Previous",
    clear_btn="Clear",
)

# Launch the interface
print("Launching Gradio interface...")
gradio_interface.launch(debug=True)

